library(tidyr)
read_csv()
library(plotly)
library(htmlwidgets)
library(readr)
install.packages(readr)
install.packages("readr")
library(dplyr)
install.packages("ggthemes")
# Begin construction of chart
fte_theme()
fte_theme
install.packages("Cairo")
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
library(xgboost)
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
install.packages("tseries")
library(tseries)
adf.test(dat, k = 10)
rm(list=ls())
library(xgboost)
library(ggplot2)
library(reshape2)
library(tseries)
setwd("C:/Users/George/Dropbox/Resume/Correlation One")
datOrig <- read.csv("stock_returns_base150.csv")
datOrig <- datOrig[1:100,]
datFull <- datOrig
rownames(datFull) <- datFull[,1]
datFull <- datFull[,-1]
dat <- datFull[1:50,]
########## Visualizing data
datOrig$date <- as.Date(datOrig$date, "%m/%d/%Y")
dat_melt <- melt(datOrig, id.var="date")
ggplot(data=dat_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_line()+
facet_grid(variable ~ ., scales="free")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
########## stationarity
adf.test(dat, k = 10)
adf.test(dat[,1], k = 10)
summary(adf.test(dat[,1], k = 10))
adf.test(dat[,1], k = 5)
adf.test(dat[,1], k = 1)
adf.test(dat[,1], k = 2)
adf.test(dat[,1], k = 3)
adf.test(dat[,1])
sapply(dat,adf.test)
sapply(dat,adf.test(k=2))
sapply(dat,adf.test,2)
sapply(dat,adf.test,k=2)
rm(list=ls())
library(xgboost)
library(ggplot2)
library(reshape2)
library(tseries)
setwd("C:/Users/George/Dropbox/Resume/Correlation One")
datOrig <- read.csv("stock_returns_base150.csv")
datOrig <- datOrig[1:100,]
datFull <- datOrig
rownames(datFull) <- datFull[,1]
datFull <- datFull[,-1]
dat <- datFull[1:50,]
########## Visualizing data
datOrig$date <- as.Date(datOrig$date, "%m/%d/%Y")
dat_melt <- melt(datOrig, id.var="date")
ggplot(data=dat_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_line()+
facet_grid(variable ~ ., scales="free")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
grangertest(S1 ~ S2 , order = norder, data = dat)
########## Variable selection
# METHOD 1: Granger Causality
library(lmtest)
norder = 4
grangertest(S1 ~ S2 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S3 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S4 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S5 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S6 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S7 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S8 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S9 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S10 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S2 , order = norder, data = dat)
grangertest(S1 ~ S2 , order = norder, data = dat)
norder = 2
grangertest(S1 ~ S2 , order = norder, data = dat)
library(leaps)
exhaustive <- regsubsets(S1~., data=dat, nbest=2, method=c("exhaustive"))
summary(exhaustive) #top two models for models with k variables, k=1:8
summary.regsubsets
summary.exhaustive
summary(exhaustive)$adjr2
str(summary(exhaustive))
rm(list=ls())
library(xgboost)
library(ggplot2)
library(reshape2)
library(tseries)
setwd("C:/Users/George/Dropbox/Resume/Correlation One")
datOrig <- read.csv("stock_returns_base150.csv")
datOrig <- datOrig[1:100,]
datFull <- datOrig
rownames(datFull) <- datFull[,1]
datFull <- datFull[,-1]
dat <- datFull[1:50,]
########## Visualizing data
datOrig$date <- as.Date(datOrig$date, "%m/%d/%Y")
dat_melt <- melt(datOrig, id.var="date")
ggplot(data=dat_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_line()+
facet_grid(variable ~ ., scales="free")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
########## stationarity
sapply(dat,adf.test,k=2)
adf.test(dat[,1], k = 10)
########## Feature Engineering
datFull_lag1 <-  datFull
datFull_lag1[2:100,] <- datFull_lag1[1:99,]
datFull_lag1[1,] <- 0
datFull_lag2 <-  datFull
datFull_lag2[3:100,] <- datFull[1:98,]
datFull_lag2[1:2,] <- 0
datFull2 <- data.frame(datFull,datFull_lag1[,-1],datFull_lag2[,-1])
datFull2 <- datFull2[3:100,]
dat <- datFull2[1:48,]
########## Variable selection
# METHOD 1: Granger Causality
library(lmtest)
norder = 4
grangertest(S1 ~ S2 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S3 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S4 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S5 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S6 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S7 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S8 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S9 , order = norder, data = dat)$"Pr(>F)"[2]
grangertest(S1 ~ S10 , order = norder, data = dat)$"Pr(>F)"[2]
# METHOD 2: Bestsubset
library(leaps)
exhaustive <- regsubsets(S1~., data=dat, nbest=2, method=c("exhaustive"))
summary(exhaustive) #top two models for models with k variables, k=1:8
plot(exhaustive, scale="adjr2")
# METHOD 3: forward stepwise selection
null=lm(S1~1, data=dat)
full=lm(S1~., data=dat)
forward<-step(null, scope=list(lower=null, upper=full), direction="forward")
summary(forward)
train <- dat[1:40,]
test <- dat[41:nrow(dat),]
str(train)
dim(train)
dim(test)
#Advanced data set format
dtrain <- xgb.DMatrix(data = as.matrix(train[,-1]), label=train[,1])
dtest <- xgb.DMatrix(data = as.matrix(test[,-1]), label=test[,1])
ddat <- xgb.DMatrix(data = as.matrix(dat[,-1]), label=dat[,1])
#Train model
watchlist <- list(train=dtrain, test=dtest)
optimizedNRound <- 23 #optimized from cross validation (see end of script)
nCores <- 4 #number of CPU cores
trainingSpeed <- 0.2 #a value from 0 to 1, lower corresponds to slower learning rate, require larger nround
treeDepth <- 2 #depth of tree, optimized from treeDepth search
#for (treeDepth in 1:6){
bst <- xgb.train(data=dtrain, max.depth=treeDepth, eta=trainingSpeed, nthread = nCores, nround=optimizedNRound, watchlist=watchlist, objective = "reg:linear")
#}
#Plot importance of features
importance_matrix <- xgb.importance(colnames(train), model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
#Insample prediction
insample_xg <- predict(bst, dtrain)
insample_forward <- predict(forward,train)
#Out of sample prediction
pred_xg <- predict(bst, dtest)
pred_forward <- predict(forward,test)
#Ensemble
insample_ensemble <- (insample_xg + insample_forward)/2
pred_ensemble <- (pred_xg + pred_forward)/2
#Visualization
pred<- data.frame(date= rownames(dat),
XG= c(insample_xg,pred_xg),
forward= c(insample_forward,pred_forward),
#ensemble= c(insample_ensemble,pred_ensemble),
actual= c(train[,1],test[,1]))
## only out of sample prediction
# pred<- data.frame(date= rownames(test),
#                   XG= c(pred_xg),
#                   forward= c(pred_forward),
#                   actual= c(test[,1]))
pred$date <- as.Date(pred$date, "%m/%d/%Y")
pred_melt <- melt(pred, id.var="date")
ggplot(data=pred_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_point()+
geom_line()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
err <- data.frame(
Insample=rbind(sqrt(mean((train[,1]-insample_xg)^2)), sqrt(mean((train[,1]-insample_forward)^2)), sqrt(mean((train[,1]-insample_ensemble)^2))),
OutOfSample=rbind(sqrt(mean((test[,1]-pred_xg)^2)), sqrt(mean((test[,1]-pred_forward)^2)), sqrt(mean((test[,1]-pred_ensemble)^2))),
row.names = c("XG Boost", "Forward", "Ensemble"))
err #XG Boost is clearly over fit
bst.cv <- xgb.cv(data=ddat, max.depth=depth, eta=eta, nthread = 4, nround=50, watchlist=watchlist, objective = "reg:linear",
nfold=5)
dpeth= 2
eta= 0.2
bst.cv <- xgb.cv(data=ddat, max.depth=depth, eta=eta, nthread = 4, nround=50, watchlist=watchlist, objective = "reg:linear",
nfold=5)
depth=2
bst.cv <- xgb.cv(data=ddat, max.depth=depth, eta=eta, nthread = 4, nround=50, watchlist=watchlist, objective = "reg:linear",
nfold=5)
plot(bst.cv[, test.rmse.mean])
#######################Final output##################
train <- datFull2[1:48,]
dtrain <- xgb.DMatrix(data = as.matrix(train[,-1]), label=train[,1])
test <- datFull2[49:98,]
dtest <- xgb.DMatrix(data = as.matrix(test[,-1]))
#Train model
bst <- xgb.train(data=dtrain, max.depth=treeDepth, eta=trainingSpeed, nthread = nCores, nround=optimizedNRound, objective = "reg:linear")
#forecast
#Insample prediction
insample_xg <- predict(bst, dtrain)
insample_forward <- predict(forward,train)
#Out of sample prediction
pred_xg <- predict(bst, dtest)
pred_forward <- predict(forward,test)
pred<- data.frame(date= rownames(datFull2),
XG= c(insample_xg,pred_xg),
forward= c(insample_forward,pred_forward),
actual = datFull2[,1])
pred$date <- as.Date(pred$date, "%m/%d/%Y")
pred_melt <- melt(pred, id.var="date")
ggplot(data=pred_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_point()+
geom_line()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#In sample error
err <- data.frame(
Insample=rbind(sqrt(mean((train[,1]-insample_xg)^2)), sqrt(mean((train[,1]-insample_forward)^2))),
OutOfSample=rbind(sqrt(mean((test[,1]-pred_xg)^2)), sqrt(mean((test[,1]-pred_forward)^2))),
row.names = c("XG Boost", "Forward"))
err
rm(list=ls())
library(xgboost)
library(ggplot2)
library(reshape2)
library(tseries)
setwd("C:/Users/George/Dropbox/Resume/Correlation One")
#./data
datOrig <- read.csv("/stock_returns_base150.csv")
datOrig <- datOrig[1:100,]
datFull <- datOrig
rownames(datFull) <- datFull[,1]
datFull <- datFull[,-1]
dat <- datFull[1:50,]
########## Visualizing data
datOrig$date <- as.Date(datOrig$date, "%m/%d/%Y")
dat_melt <- melt(datOrig, id.var="date")
ggplot(data=dat_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_line()+
facet_grid(variable ~ ., scales="free")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
########## stationarity
sapply(dat,adf.test,k=2)
adf.test(dat[,1], k = 10)
########## Feature Engineering
datFull_lag1 <-  datFull
datFull_lag1[2:100,] <- datFull_lag1[1:99,]
datFull_lag1[1,] <- 0
datFull_lag2 <-  datFull
datFull_lag2[3:100,] <- datFull[1:98,]
datFull_lag2[1:2,] <- 0
datFull2 <- data.frame(datFull,datFull_lag1[,-1],datFull_lag2[,-1])
datFull2 <- datFull2[3:100,]
dat <- datFull2[1:48,]
###########Separate into training and testing data set for validation
train <- dat[1:40,]
test <- dat[41:nrow(dat),]
str(train)
dim(train)
dim(test)
########## Variable selection
# METHOD 1: Granger Causality
library(lmtest)
norder = 4
grangertest(S1 ~ S2 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S3 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S4 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S5 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S6 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S7 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S8 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S9 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S10 , order = norder, data = train)$"Pr(>F)"[2]
# METHOD 2: Bestsubset
library(leaps)
exhaustive <- regsubsets(S1~., data=train, nbest=2, method=c("exhaustive"))
summary(exhaustive) #top two models for models with k variables, k=1:8
plot(exhaustive, scale="adjr2")
# METHOD 3: forward stepwise selection
null=lm(S1~1, data=train)
full=lm(S1~., data=train)
forward<-step(null, scope=list(lower=null, upper=full), direction="forward")
summary(forward)
rm(list=ls())
library(xgboost)
library(ggplot2)
library(reshape2)
library(tseries)
setwd("C:/Users/George/Dropbox/Resume/Correlation One")
#./data/
datOrig <- read.csv("stock_returns_base150.csv")
datOrig <- datOrig[1:100,]
datFull <- datOrig
rownames(datFull) <- datFull[,1]
datFull <- datFull[,-1]
dat <- datFull[1:50,]
########## Visualizing data
datOrig$date <- as.Date(datOrig$date, "%m/%d/%Y")
dat_melt <- melt(datOrig, id.var="date")
ggplot(data=dat_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_line()+
facet_grid(variable ~ ., scales="free")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
########## stationarity
sapply(dat,adf.test,k=2)
adf.test(dat[,1], k = 10)
########## Feature Engineering
datFull_lag1 <-  datFull
datFull_lag1[2:100,] <- datFull_lag1[1:99,]
datFull_lag1[1,] <- 0
datFull_lag2 <-  datFull
datFull_lag2[3:100,] <- datFull[1:98,]
datFull_lag2[1:2,] <- 0
datFull2 <- data.frame(datFull,datFull_lag1[,-1],datFull_lag2[,-1])
datFull2 <- datFull2[3:100,]
dat <- datFull2[1:48,]
###########Separate into training and testing data set for validation
train <- dat[1:40,]
test <- dat[41:nrow(dat),]
str(train)
dim(train)
dim(test)
########## Variable selection
# METHOD 1: Granger Causality
library(lmtest)
norder = 4
grangertest(S1 ~ S2 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S3 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S4 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S5 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S6 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S7 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S8 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S9 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S10 , order = norder, data = train)$"Pr(>F)"[2]
# METHOD 2: Bestsubset
library(leaps)
exhaustive <- regsubsets(S1~., data=train, nbest=2, method=c("exhaustive"))
summary(exhaustive) #top two models for models with k variables, k=1:8
plot(exhaustive, scale="adjr2")
# METHOD 3: forward stepwise selection
null=lm(S1~1, data=train)
full=lm(S1~., data=train)
forward<-step(null, scope=list(lower=null, upper=full), direction="forward")
summary(forward)
#XGBoost data set format
dtrain <- xgb.DMatrix(data = as.matrix(train[,-1]), label=train[,1])
dtest <- xgb.DMatrix(data = as.matrix(test[,-1]), label=test[,1])
ddat <- xgb.DMatrix(data = as.matrix(dat[,-1]), label=dat[,1])
#Train model
watchlist <- list(train=dtrain, test=dtest)
optimizedNRound <- 23 #optimized from cross validation (see end of script)
nCores <- 4 #number of CPU cores
trainingSpeed <- 0.2 #a value from 0 to 1, lower corresponds to slower learning rate, require larger nround
treeDepth <- 2 #depth of tree, optimized from treeDepth search
#for (treeDepth in 1:6){
bst <- xgb.train(data=dtrain, max.depth=treeDepth, eta=trainingSpeed, nthread = nCores, nround=optimizedNRound, watchlist=watchlist, objective = "reg:linear")
#}
#Plot importance of features
importance_matrix <- xgb.importance(colnames(train), model = bst)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)
#Insample prediction
insample_xg <- predict(bst, dtrain)
insample_forward <- predict(forward,train)
#Out of sample prediction
pred_xg <- predict(bst, dtest)
pred_forward <- predict(forward,test)
#Ensemble
insample_ensemble <- (insample_xg + insample_forward)/2
pred_ensemble <- (pred_xg + pred_forward)/2
#Visualization
pred<- data.frame(date= rownames(dat),
XG= c(insample_xg,pred_xg),
forward= c(insample_forward,pred_forward),
#ensemble= c(insample_ensemble,pred_ensemble),
actual= c(train[,1],test[,1]))
## only out of sample prediction
# pred<- data.frame(date= rownames(test),
#                   XG= c(pred_xg),
#                   forward= c(pred_forward),
#                   actual= c(test[,1]))
pred$date <- as.Date(pred$date, "%m/%d/%Y")
pred_melt <- melt(pred, id.var="date")
ggplot(data=pred_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_point()+
geom_line()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#################Error calculation
err <- data.frame(
Insample=rbind(sqrt(mean((train[,1]-insample_xg)^2)), sqrt(mean((train[,1]-insample_forward)^2)), sqrt(mean((train[,1]-insample_ensemble)^2))),
OutOfSample=rbind(sqrt(mean((test[,1]-pred_xg)^2)), sqrt(mean((test[,1]-pred_forward)^2)), sqrt(mean((test[,1]-pred_ensemble)^2))),
row.names = c("XG Boost", "Forward", "Ensemble"))
err #XG Boost is clearly over fit
xgb.plot.importance(importance_matrix = importance_matrix)
#######################Final output##################
train <- datFull2[1:48,]
dtrain <- xgb.DMatrix(data = as.matrix(train[,-1]), label=train[,1])
test <- datFull2[49:98,]
dtest <- xgb.DMatrix(data = as.matrix(test[,-1]))
#Train model
bst <- xgb.train(data=dtrain, max.depth=treeDepth, eta=trainingSpeed, nthread = nCores, nround=optimizedNRound, objective = "reg:linear")
#forecast
#Insample prediction
insample_xg <- predict(bst, dtrain)
insample_forward <- predict(forward,train)
#Out of sample prediction
pred_xg <- predict(bst, dtest)
pred_forward <- predict(forward,test)
pred<- data.frame(date= rownames(datFull2),
XG= c(insample_xg,pred_xg),
forward= c(insample_forward,pred_forward),
actual = datFull2[,1])
pred$date <- as.Date(pred$date, "%m/%d/%Y")
pred_melt <- melt(pred, id.var="date")
ggplot(data=pred_melt, aes(x=date, y=value, group=variable, color=variable)) +
geom_point()+
geom_line()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#In sample error
err <- data.frame(
Insample=rbind(sqrt(mean((train[,1]-insample_xg)^2)), sqrt(mean((train[,1]-insample_forward)^2))),
OutOfSample=rbind(sqrt(mean((test[,1]-pred_xg)^2)), sqrt(mean((test[,1]-pred_forward)^2))),
row.names = c("XG Boost", "Forward"))
err
pred_xg
rownames(test)
output <- data.frame(Date=rownames(test),Value=pred_xg)
output
write.csv(output,'predictions.csv')
write.csv(output,'predictions.csv', row.names = F)
write.csv(output,'predictions.csv', row.names = F)
norder = 4
grangertest(S1 ~ S2 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S3 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S4 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S5 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S6 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S7 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S8 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S9 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S10 , order = norder, data = train)$"Pr(>F)"[2]
library(lmtest)
norder = 4
grangertest(S1 ~ S2 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S3 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S4 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S5 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S6 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S7 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S8 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S9 , order = norder, data = train)$"Pr(>F)"[2]
grangertest(S1 ~ S10 , order = norder, data = train)$"Pr(>F)"[2]
